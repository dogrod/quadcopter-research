{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wi-Fi Network Security Threat Detection Using Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the pcap files and the output directory\n",
    "pcap_directory = \"../datasets/wifi\"\n",
    "output_directory = \"./model_output/wifi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scapy.utils import rdpcap\n",
    "from scapy.layers.inet import IP\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, roc_auc_score, precision_recall_curve,\n",
    "                           average_precision_score, confusion_matrix,\n",
    "                           roc_curve, auc, silhouette_score)\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file\n",
    "df = pd.read_csv('../datasets/wifi/output_1201_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10551408, 24)\n",
      "   frame.time_epoch  frame.len              frame.protocols  wlan.fc.type  \\\n",
      "0      1.729577e+09       1363  eth:ethertype:ipv6:udp:mdns           NaN   \n",
      "1      1.729577e+09         66         eth:ethertype:ip:tcp           NaN   \n",
      "2      1.729577e+09         54         eth:ethertype:ip:tcp           NaN   \n",
      "3      1.729577e+09         42            eth:ethertype:arp           NaN   \n",
      "4      1.729577e+09         83    eth:ethertype:ip:udp:data           NaN   \n",
      "\n",
      "   wlan.fc.subtype wlan.sa wlan.da wlan.bssid  radiotap.channel.freq  \\\n",
      "0              NaN     NaN     NaN        NaN                    NaN   \n",
      "1              NaN     NaN     NaN        NaN                    NaN   \n",
      "2              NaN     NaN     NaN        NaN                    NaN   \n",
      "3              NaN     NaN     NaN        NaN                    NaN   \n",
      "4              NaN     NaN     NaN        NaN                    NaN   \n",
      "\n",
      "   radiotap.dbm_antsignal  ...  tcp.dstport udp.srcport udp.dstport  \\\n",
      "0                     NaN  ...          NaN      5353.0      5353.0   \n",
      "1                     NaN  ...      49472.0         NaN         NaN   \n",
      "2                     NaN  ...      49926.0         NaN         NaN   \n",
      "3                     NaN  ...          NaN         NaN         NaN   \n",
      "4                     NaN  ...          NaN     58578.0      7844.0   \n",
      "\n",
      "   tcp.flags  tcp.len  udp.length  tcp.stream tcp.seq  tcp.ack  \\\n",
      "0        NaN      NaN      1309.0         NaN     NaN      NaN   \n",
      "1     0x0010      0.0         NaN         0.0     1.0      1.0   \n",
      "2     0x0014      0.0         NaN         1.0     1.0      1.0   \n",
      "3        NaN      NaN         NaN         NaN     NaN      NaN   \n",
      "4        NaN      NaN        49.0         NaN     NaN      NaN   \n",
      "\n",
      "   _ws.col.protocol  \n",
      "0              MDNS  \n",
      "1               TCP  \n",
      "2               TCP  \n",
      "3               ARP  \n",
      "4               UDP  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "Index(['frame.time_epoch', 'frame.len', 'frame.protocols', 'wlan.fc.type',\n",
      "       'wlan.fc.subtype', 'wlan.sa', 'wlan.da', 'wlan.bssid',\n",
      "       'radiotap.channel.freq', 'radiotap.dbm_antsignal', 'radiotap.datarate',\n",
      "       'ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport', 'udp.srcport',\n",
      "       'udp.dstport', 'tcp.flags', 'tcp.len', 'udp.length', 'tcp.stream',\n",
      "       'tcp.seq', 'tcp.ack', '_ws.col.protocol'],\n",
      "      dtype='object')\n",
      "frame.time_epoch                 0\n",
      "frame.len                        0\n",
      "frame.protocols                  0\n",
      "wlan.fc.type              10551403\n",
      "wlan.fc.subtype           10551403\n",
      "wlan.sa                   10551403\n",
      "wlan.da                   10551403\n",
      "wlan.bssid                10551403\n",
      "radiotap.channel.freq     10551408\n",
      "radiotap.dbm_antsignal    10551408\n",
      "radiotap.datarate         10551408\n",
      "ip.src                      269246\n",
      "ip.dst                      269246\n",
      "tcp.srcport                 952554\n",
      "tcp.dstport                 952554\n",
      "udp.srcport                9644094\n",
      "udp.dstport                9644094\n",
      "tcp.flags                   952554\n",
      "tcp.len                     953080\n",
      "udp.length                 9644094\n",
      "tcp.stream                  952554\n",
      "tcp.seq                     952554\n",
      "tcp.ack                     952554\n",
      "_ws.col.protocol                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "\n",
    "class NetworkFeatureExtractor:\n",
    "    def __init__(self, window: str = '1Min', chunk_size: int = 100000):\n",
    "        self.window = window\n",
    "        self.chunk_size = chunk_size\n",
    "        self.scaler = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Optimize DataFrame memory usage by adjusting data types.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Ensure protocol column is string and non-empty\n",
    "        if '_ws.col.protocol' in df.columns:\n",
    "            df['_ws.col.protocol'] = df['_ws.col.protocol'].fillna('unknown').astype(str)\n",
    "            df.loc[df['_ws.col.protocol'].str.len() == 0, '_ws.col.protocol'] = 'unknown'\n",
    "        \n",
    "        # Convert numeric columns\n",
    "        int_columns = ['frame.len', 'tcp.srcport', 'tcp.dstport', 'udp.srcport', 'udp.dstport']\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], downcast='integer', errors='ignore')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def extract_flags(self, flag_str) -> Dict[str, int]:\n",
    "        \"\"\"Extract TCP flags into individual boolean features.\"\"\"\n",
    "        flags = {\n",
    "            'SYN': 0, 'ACK': 0, 'PSH': 0, 'RST': 0, 'FIN': 0, 'URG': 0\n",
    "        }\n",
    "        if pd.isna(flag_str):\n",
    "            return flags\n",
    "        for flag in flags.keys():\n",
    "            flags[flag] = 1 if flag in str(flag_str) else 0\n",
    "        return flags\n",
    "\n",
    "    def create_basic_features(self, chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create basic traffic features with minimal memory usage.\"\"\"\n",
    "        features = chunk.resample(self.window, on='datetime').agg({\n",
    "            'frame.len': ['count', 'mean', 'sum'],\n",
    "            'ip.src': 'nunique',\n",
    "            'ip.dst': 'nunique',\n",
    "            '_ws.col.protocol': 'nunique'\n",
    "        })\n",
    "        \n",
    "        # Flatten column names\n",
    "        features.columns = ['_'.join(col).strip() for col in features.columns.values]\n",
    "        return features\n",
    "    \n",
    "    def add_protocol_features(self, chunk: pd.DataFrame, base_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add protocol-specific features using a more direct approach.\"\"\"\n",
    "        # Ensure working with a copy and proper types\n",
    "        chunk = chunk.copy()\n",
    "        chunk['_ws.col.protocol'] = chunk['_ws.col.protocol'].fillna('unknown').astype(str)\n",
    "        \n",
    "        # Create time bins first\n",
    "        chunk['time_bin'] = chunk['datetime'].dt.floor(self.window)\n",
    "        \n",
    "        # Get unique protocols\n",
    "        protocols = chunk['_ws.col.protocol'].unique()\n",
    "        \n",
    "        # Calculate protocol counts for each time bin\n",
    "        for protocol in protocols:\n",
    "            mask = chunk['_ws.col.protocol'] == protocol\n",
    "            counts = chunk[mask].groupby('time_bin').size()\n",
    "            base_features[f'protocol_{protocol}_count'] = counts\n",
    "        \n",
    "        # Fill missing values\n",
    "        base_features = base_features.fillna(0)\n",
    "        \n",
    "        # Calculate ratios\n",
    "        protocol_columns = [col for col in base_features.columns if col.startswith('protocol_') and col.endswith('_count')]\n",
    "        total_counts = base_features[protocol_columns].sum(axis=1)\n",
    "        \n",
    "        for protocol in protocols:\n",
    "            count_col = f'protocol_{protocol}_count'\n",
    "            if count_col in base_features.columns:\n",
    "                base_features[f'protocol_{protocol}_ratio'] = base_features[count_col] / (total_counts + 1e-10)\n",
    "        \n",
    "        return base_features\n",
    "    \n",
    "    def add_flag_features(self, chunk: pd.DataFrame, base_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add TCP flag-based features.\"\"\"\n",
    "        if 'tcp.flags' in chunk.columns:\n",
    "            # Instead of creating a dictionary, process flags directly\n",
    "            flags = ['SYN', 'ACK', 'PSH', 'RST', 'FIN', 'URG']\n",
    "            \n",
    "            for flag in flags:\n",
    "                # Create boolean column for each flag\n",
    "                flag_present = chunk['tcp.flags'].fillna('').str.contains(flag).astype(int)\n",
    "                \n",
    "                # Resample and sum for each flag\n",
    "                flag_counts = chunk.assign(**{f'flag_{flag}': flag_present})\\\n",
    "                    .resample(self.window, on='datetime')[f'flag_{flag}'].sum()\n",
    "                \n",
    "                base_features[f'tcp_flag_{flag}_count'] = flag_counts\n",
    "            \n",
    "            # Add flag ratios\n",
    "            base_features['syn_ack_ratio'] = base_features['tcp_flag_SYN_count'] / \\\n",
    "                (base_features['tcp_flag_ACK_count'] + 1)\n",
    "            base_features['reset_syn_ratio'] = base_features['tcp_flag_RST_count'] / \\\n",
    "                (base_features['tcp_flag_SYN_count'] + 1)\n",
    "        \n",
    "        return base_features\n",
    "    \n",
    "    def process_chunk(self, chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process a single chunk of data.\"\"\"\n",
    "        try:\n",
    "            # Create basic features\n",
    "            features = self.create_basic_features(chunk)\n",
    "            \n",
    "            # Add additional feature sets\n",
    "            features = self.add_protocol_features(chunk, features)\n",
    "            features = self.add_flag_features(chunk, features)\n",
    "            features = self.add_port_features(chunk, features)\n",
    "            features = self.add_statistical_features(features)\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_chunk: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def add_port_features(self, chunk: pd.DataFrame, base_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add port-related features.\"\"\"\n",
    "        # Unique ports accessed\n",
    "        base_features['unique_ports_accessed'] = chunk.resample(self.window, on='datetime').apply(\n",
    "            lambda x: len(set(x['tcp.dstport'].dropna()) | set(x['udp.dstport'].dropna()))\n",
    "        )\n",
    "        \n",
    "        return base_features\n",
    "    \n",
    "    def add_statistical_features(self, base_features: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add statistical and rate-based features.\"\"\"\n",
    "        for col in ['frame.len_count', 'ip.src_nunique', 'ip.dst_nunique']:\n",
    "            if col in base_features.columns:\n",
    "                roll = base_features[col].rolling(window=5, min_periods=1)\n",
    "                base_features[f'{col}_rolling_mean'] = roll.mean().fillna(0)\n",
    "                base_features[f'{col}_rate'] = base_features[col].diff().fillna(0)\n",
    "        \n",
    "        # Traffic ratios\n",
    "        base_features['bytes_per_packet'] = base_features['frame.len_sum'] / \\\n",
    "            (base_features['frame.len_count'] + 1)\n",
    "        base_features['packets_per_src'] = base_features['frame.len_count'] / \\\n",
    "            (base_features['ip.src_nunique'] + 1)\n",
    "        \n",
    "        return base_features\n",
    "    \n",
    "    def preprocess_features(self, features: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"Preprocess features for anomaly detection.\"\"\"\n",
    "        features_prep = features.copy()\n",
    "        \n",
    "        # Handle infinite values and NaN\n",
    "        features_prep = features_prep.replace([np.inf, -np.inf], np.nan)\n",
    "        features_prep = features_prep.fillna(0)\n",
    "        \n",
    "        # Remove constant and zero variance columns\n",
    "        constant_cols = [col for col in features_prep.columns if features_prep[col].nunique() == 1]\n",
    "        zero_var_cols = features_prep.columns[features_prep.std() == 0]\n",
    "        cols_to_drop = list(set(constant_cols + list(zero_var_cols)))\n",
    "        \n",
    "        if cols_to_drop:\n",
    "            features_prep = features_prep.drop(columns=cols_to_drop)\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = RobustScaler()\n",
    "        scaled_features = self.scaler.fit_transform(features_prep)\n",
    "        \n",
    "        # Final NaN check\n",
    "        scaled_features = np.nan_to_num(scaled_features, 0)\n",
    "        \n",
    "        return scaled_features, features_prep.columns.tolist()\n",
    "    \n",
    "    def save_processed_data(self, scaled_features: np.ndarray, feature_columns: List[str], \n",
    "                          raw_features: pd.DataFrame, output_dir: str = output_directory):\n",
    "        \"\"\"Save processed data to CSV files.\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # Save scaled features\n",
    "        scaled_df = pd.DataFrame(scaled_features, columns=feature_columns, \n",
    "                               index=raw_features.index)\n",
    "        scaled_df.to_csv(f'{output_dir}/scaled_features.csv')\n",
    "        \n",
    "        # Save raw features\n",
    "        raw_features.to_csv(f'{output_dir}/raw_features.csv')\n",
    "\n",
    "        # Save scaler\n",
    "        joblib.dump(self.scaler, f'{output_dir}/scaler.joblib')\n",
    "        \n",
    "        print(f\"Saved processed data to {output_dir}/\")\n",
    "        print(f\"Scaled features shape: {scaled_df.shape}\")\n",
    "        print(f\"Raw features shape: {raw_features.shape}\")\n",
    "        print(f\"Scaler saved to {output_dir}/scaler.joblib\")\n",
    "\n",
    "    def convert_categorical_to_string(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Convert all categorical columns to string type.\"\"\"\n",
    "        df = df.copy()\n",
    "        categorical_columns = df.select_dtypes(include=['category']).columns\n",
    "        \n",
    "        for col in categorical_columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def process_data(self, df: pd.DataFrame, save_output: bool = True) -> Tuple[np.ndarray, List[str], pd.DataFrame]:\n",
    "        \"\"\"Main processing function.\"\"\"\n",
    "        try:\n",
    "            print(\"Starting feature extraction...\")\n",
    "\n",
    "            # Convert categorical columns to string first\n",
    "            df = self.convert_categorical_to_string(df)\n",
    "            \n",
    "            # Convert timestamp and optimize dtypes\n",
    "            df['datetime'] = pd.to_datetime(df['frame.time_epoch'], unit='s')\n",
    "            df = self.optimize_dtypes(df)\n",
    "            \n",
    "            # Process data in chunks\n",
    "            chunks = [df[i:i + self.chunk_size] for i in range(0, len(df), self.chunk_size)]\n",
    "            all_features = []\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "                chunk_features = self.process_chunk(chunk)\n",
    "                all_features.append(chunk_features)\n",
    "            \n",
    "            # Combine all features\n",
    "            combined_features = pd.concat(all_features)\n",
    "            \n",
    "            # Preprocess features\n",
    "            scaled_features, feature_columns = self.preprocess_features(combined_features)\n",
    "            \n",
    "            if save_output:\n",
    "                self.save_processed_data(scaled_features, feature_columns, combined_features)\n",
    "            \n",
    "            return scaled_features, feature_columns, combined_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during processing: {e}\")\n",
    "            raise\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_processed_data(input_dir: str = 'processed_data') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load processed data from CSV files.\"\"\"\n",
    "        scaled_df = pd.read_csv(f'{input_dir}/scaled_features.csv', index_col=0)\n",
    "        raw_df = pd.read_csv(f'{input_dir}/raw_features.csv', index_col=0)\n",
    "        \n",
    "        print(f\"Loaded processed data from {input_dir}/\")\n",
    "        print(f\"Scaled features shape: {scaled_df.shape}\")\n",
    "        print(f\"Raw features shape: {raw_df.shape}\")\n",
    "        \n",
    "        return scaled_df, raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction...\n",
      "Processing chunk 1/106...\n",
      "Error during processing: '<lambda_0>'\n",
      "Error: '<lambda_0>'\n"
     ]
    }
   ],
   "source": [
    "extractor = NetworkFeatureExtractor(window='1Min', chunk_size=100000)\n",
    "\n",
    "try:\n",
    "    # Process data\n",
    "    extractor.process_data(df)\n",
    "    \n",
    "    # Or load previously processed data\n",
    "    # scaled_df, raw_df = NetworkFeatureExtractor.load_processed_data()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
