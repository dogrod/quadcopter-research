{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wi-Fi Network Security Threat Detection Using Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the pcap files and the output directory\n",
    "pcap_directory = \"../datasets/wifi\"\n",
    "output_directory = \"./model_output/wifi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scapy.utils import rdpcap\n",
    "from scapy.layers.inet import IP\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, roc_auc_score, precision_recall_curve,\n",
    "                           average_precision_score, confusion_matrix,\n",
    "                           roc_curve, auc, silhouette_score)\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "\n",
    "def load_processed_data(input_dir: str = 'processed_data') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "  \"\"\"Load processed data from CSV files.\"\"\"\n",
    "  scaled_df = pd.read_csv(f'{input_dir}/scaled_features.csv', index_col=0)\n",
    "  raw_df = pd.read_csv(f'{input_dir}/raw_features.csv', index_col=0)\n",
    "  \n",
    "  print(f\"Loaded processed data from {input_dir}/\")\n",
    "  print(f\"Scaled features shape: {scaled_df.shape}\")\n",
    "  print(f\"Raw features shape: {raw_df.shape}\")\n",
    "  \n",
    "  return scaled_df, raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed data from processed_data/\n",
      "Scaled features shape: (39053, 403)\n",
      "Raw features shape: (39053, 411)\n"
     ]
    }
   ],
   "source": [
    "X_scaled, raw_features = load_processed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 23:28:17.043124: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-01 23:28:17.103565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733124497.109332   29322 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733124497.111009   29322 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 23:28:17.118665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def hyperparameter_tuning_iso_forest(X):\n",
    "    \"\"\"\n",
    "    Manually tune Isolation Forest hyperparameters using unsupervised metrics.\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_samples': ['auto', 0.5, 0.75],\n",
    "        'contamination': [0.01, 0.05, 0.1],\n",
    "        'max_features': [1.0, 0.5, 0.75]\n",
    "    }\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_samples in param_grid['max_samples']:\n",
    "            for contamination in param_grid['contamination']:\n",
    "                for max_features in param_grid['max_features']:\n",
    "                    model = IsolationForest(\n",
    "                        n_estimators=n_estimators,\n",
    "                        max_samples=max_samples,\n",
    "                        contamination=contamination,\n",
    "                        max_features=max_features,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    model.fit(X)\n",
    "                    scores = -model.decision_function(X)\n",
    "                    \n",
    "                    # Evaluate using Silhouette Score\n",
    "                    threshold = np.percentile(scores, 100 * (1 - contamination))\n",
    "                    labels = (scores >= threshold).astype(int)\n",
    "                    \n",
    "                    if len(np.unique(labels)) > 1:\n",
    "                        silhouette = silhouette_score(X, labels)\n",
    "                    else:\n",
    "                        silhouette = -1  # Assign a poor score if only one cluster\n",
    "                    \n",
    "                    if silhouette > best_score:\n",
    "                        best_score = silhouette\n",
    "                        best_params = {\n",
    "                            'n_estimators': n_estimators,\n",
    "                            'max_samples': max_samples,\n",
    "                            'contamination': contamination,\n",
    "                            'max_features': max_features\n",
    "                        }\n",
    "                        best_model = model\n",
    "                        \n",
    "    print(\"Best parameters for Isolation Forest:\")\n",
    "    print(best_params)\n",
    "    print(f\"Best Silhouette Score: {best_score:.4f}\")\n",
    "    \n",
    "    # Get final scores with best model\n",
    "    final_scores = -best_model.decision_function(X)\n",
    "    \n",
    "    return best_model, final_scores\n",
    "\n",
    "def hyperparameter_tuning_lof(X):\n",
    "    \"\"\"\n",
    "    Manually tune LOF hyperparameters using unsupervised metrics.\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'n_neighbors': [10, 20, 30],\n",
    "        'leaf_size': [30, 50],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    for n_neighbors in param_grid['n_neighbors']:\n",
    "        for leaf_size in param_grid['leaf_size']:\n",
    "            for metric in param_grid['metric']:\n",
    "                model = LocalOutlierFactor(\n",
    "                    n_neighbors=n_neighbors,\n",
    "                    leaf_size=leaf_size,\n",
    "                    metric=metric,\n",
    "                    novelty=True\n",
    "                )\n",
    "                model.fit(X)\n",
    "                scores = -model.decision_function(X)\n",
    "                \n",
    "                # Evaluate using Silhouette Score\n",
    "                contamination = 0.05  # Assume 5% anomalies\n",
    "                threshold = np.percentile(scores, 100 * (1 - contamination))\n",
    "                labels = (scores >= threshold).astype(int)\n",
    "                \n",
    "                if len(np.unique(labels)) > 1:\n",
    "                    silhouette = silhouette_score(X, labels)\n",
    "                else:\n",
    "                    silhouette = -1  # Assign a poor score if only one cluster\n",
    "                \n",
    "                if silhouette > best_score:\n",
    "                    best_score = silhouette\n",
    "                    best_params = {\n",
    "                        'n_neighbors': n_neighbors,\n",
    "                        'leaf_size': leaf_size,\n",
    "                        'metric': metric\n",
    "                    }\n",
    "                    best_model = model\n",
    "                        \n",
    "    print(\"Best parameters for LOF:\")\n",
    "    print(best_params)\n",
    "    print(f\"Best Silhouette Score: {best_score:.4f}\")\n",
    "    \n",
    "    # Get final scores with best model\n",
    "    final_scores = -best_model.decision_function(X)\n",
    "    \n",
    "    return best_model, final_scores\n",
    "\n",
    "import kerastuner as kt\n",
    "import json\n",
    "def build_autoencoder_model(hp):\n",
    "    \"\"\"\n",
    "    Build autoencoder with dynamic input dimension handling\n",
    "    \"\"\"\n",
    "    input_dim = X_scaled.shape[1]\n",
    "    \n",
    "    # Ensure consistent layer sizes\n",
    "    encoding_dims = [\n",
    "        hp.Int('encoding_' + str(i), \n",
    "               min_value=32,\n",
    "               max_value=input_dim,\n",
    "               step=32)\n",
    "        for i in range(hp.Int('num_encoding_layers', 1, 3))\n",
    "    ]\n",
    "    \n",
    "    # Build model\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = input_layer\n",
    "    \n",
    "    # Encoder\n",
    "    for dim in encoding_dims:\n",
    "        encoded = Dense(dim, activation='relu')(encoded)\n",
    "        if hp.Boolean('use_dropout'):\n",
    "            encoded = Dropout(hp.Float('dropout_rate', 0.1, 0.5))(encoded)\n",
    "            \n",
    "    # Decoder (mirror encoder architecture)\n",
    "    decoded = encoded\n",
    "    for dim in reversed(encoding_dims[:-1]):\n",
    "        decoded = Dense(dim, activation='relu')(decoded)\n",
    "    \n",
    "    # Final output layer must match input dimension\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "    \n",
    "    # Create and compile model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='mse'\n",
    "    )\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "def train_autoencoder_tuned(X):\n",
    "    \"\"\"\n",
    "    Train autoencoder with improved dimension handling\n",
    "    \"\"\"\n",
    "    # Create tuner\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_autoencoder_model,\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        directory='autoencoder_tuning',\n",
    "        project_name='wifi_anomaly_detection',\n",
    "        overwrite=True  # Add this to avoid conflicts\n",
    "    )\n",
    "    \n",
    "    # Add early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Fit tuner\n",
    "    try:\n",
    "        tuner.search(\n",
    "            X, X,  # Input = Output for autoencoder\n",
    "            epochs=20,\n",
    "            batch_size=32,  # Reduced batch size\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Get best model\n",
    "        best_params = tuner.get_best_hyperparameters(1)[0]\n",
    "        best_model = tuner.hypermodel.build(best_params)\n",
    "        \n",
    "        # Train best model\n",
    "        best_model.fit(\n",
    "            X, X,\n",
    "            epochs=30,  # Increase epochs for final training\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save hyperparameters\n",
    "        with open(\"best_autoencoder_params.json\", \"w\") as f:\n",
    "            json.dump(best_params.values, f)\n",
    "            \n",
    "        # Calculate reconstruction errors\n",
    "        reconstructed_X = best_model.predict(X)\n",
    "        mse = np.mean(np.power(X - reconstructed_X, 2), axis=1)\n",
    "        \n",
    "        return best_model, mse\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during autoencoder training: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Isolation Forest:\n",
      "{'n_estimators': 100, 'max_samples': 0.5, 'contamination': 0.01, 'max_features': 0.5}\n",
      "Best Silhouette Score: 0.9863\n"
     ]
    }
   ],
   "source": [
    "features_scaled_df = X_scaled.copy()\n",
    "\n",
    "# Train Isolation Forest\n",
    "iso_forest_model, iso_scores = hyperparameter_tuning_iso_forest(X_scaled)\n",
    "\n",
    "# Add scores to the data\n",
    "features_scaled_df['iso_scores'] = iso_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LOF:\n",
      "{'n_neighbors': 10, 'leaf_size': 30, 'metric': 'manhattan'}\n",
      "Best Silhouette Score: 0.9855\n"
     ]
    }
   ],
   "source": [
    "# Train Local Outlier Factor\n",
    "lof_model, lof_scores = hyperparameter_tuning_lof(X_scaled)\n",
    "\n",
    "# Add scores to the data\n",
    "features_scaled_df['lof_scores'] = lof_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 06s]\n",
      "val_loss: 908835.6875\n",
      "\n",
      "Best val_loss So Far: 708745.25\n",
      "Total elapsed time: 00h 00m 56s\n",
      "Epoch 1/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 208006240.0000 - val_loss: 1043362624.0000\n",
      "Epoch 2/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 43972780.0000 - val_loss: 2206575616.0000\n",
      "Epoch 3/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 134839488.0000 - val_loss: 92033608.0000\n",
      "Epoch 4/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 132504696.0000 - val_loss: 15877395.0000\n",
      "Epoch 5/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 9922196.0000 - val_loss: 79376032.0000\n",
      "Epoch 6/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 23715012.0000 - val_loss: 967909.0000\n",
      "Epoch 7/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 23488342.0000 - val_loss: 747311.2500\n",
      "Epoch 8/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 17470480.0000 - val_loss: 1169607.8750\n",
      "Epoch 9/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 6477840.0000 - val_loss: 2778380.2500\n",
      "Epoch 10/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3250808.0000 - val_loss: 12476263.0000\n",
      "Epoch 11/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 15030168.0000 - val_loss: 57185960.0000\n",
      "Epoch 12/30\n",
      "\u001b[1m977/977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1945142.1250 - val_loss: 2190675456.0000\n",
      "\u001b[1m1221/1221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 582us/step\n"
     ]
    }
   ],
   "source": [
    "# Train Autoencoder\n",
    "autoencoder_model, ae_scores = train_autoencoder_tuned(X_scaled)\n",
    "\n",
    "# Add reconstruction errors to the data\n",
    "features_scaled_df['ae_scores '] = ae_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_model_unsupervised(X, scores, n_clusters=3, method='quantile'):\n",
    "    \"\"\"\n",
    "    Evaluate the model using unsupervised metrics with improved robustness.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        The input samples to evaluate\n",
    "    scores : array-like of shape (n_samples,)\n",
    "        The anomaly scores from the model\n",
    "    n_clusters : int, default=3\n",
    "        Number of clusters to create from scores\n",
    "    method : str, default='quantile'\n",
    "        Method to create clusters: 'quantile' or 'kmeans'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Evaluation metrics and diagnostic information\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    X = np.array(X)\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    if len(X) != len(scores):\n",
    "        raise ValueError(f\"Length mismatch: X ({len(X)}) and scores ({len(scores)})\")\n",
    "        \n",
    "    # Scale the features for better metric calculation\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Create clusters using quantiles\n",
    "    if method == 'quantile':\n",
    "        quantiles = np.linspace(0, 100, n_clusters+1)\n",
    "        thresholds = np.percentile(scores, quantiles[1:-1])\n",
    "        labels = np.zeros(len(scores), dtype=int)\n",
    "        \n",
    "        for i, threshold in enumerate(thresholds, 1):\n",
    "            labels[scores >= threshold] = i\n",
    "    \n",
    "    # Validate clusters\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_unique_clusters = len(unique_labels)\n",
    "    \n",
    "    if n_unique_clusters < 2:\n",
    "        return {\n",
    "            'error': 'Insufficient distinct clusters formed',\n",
    "            'n_clusters_formed': n_unique_clusters,\n",
    "            'cluster_sizes': {label: np.sum(labels == label) for label in unique_labels},\n",
    "            'score_range': {'min': scores.min(), 'max': scores.max()}\n",
    "        }\n",
    "    \n",
    "    # Calculate metrics with error handling\n",
    "    metrics = {}\n",
    "    try:\n",
    "        metrics['silhouette_score'] = silhouette_score(X_scaled, labels)\n",
    "    except Exception as e:\n",
    "        metrics['silhouette_score'] = {'error': str(e)}\n",
    "        \n",
    "    try:\n",
    "        metrics['calinski_harabasz_score'] = calinski_harabasz_score(X_scaled, labels)\n",
    "    except Exception as e:\n",
    "        metrics['calinski_harabasz_score'] = {'error': str(e)}\n",
    "        \n",
    "    try:\n",
    "        metrics['davies_bouldin_score'] = davies_bouldin_score(X_scaled, labels)\n",
    "    except Exception as e:\n",
    "        metrics['davies_bouldin_score'] = {'error': str(e)}\n",
    "    \n",
    "    # Add diagnostic information\n",
    "    metrics['diagnostics'] = {\n",
    "        'n_clusters': n_unique_clusters,\n",
    "        'cluster_sizes': {label: np.sum(labels == label) for label in unique_labels},\n",
    "        'cluster_proportions': {label: np.mean(labels == label) for label in unique_labels},\n",
    "        'score_distribution': {\n",
    "            'min': scores.min(),\n",
    "            'max': scores.max(),\n",
    "            'mean': scores.mean(),\n",
    "            'std': scores.std()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest Metrics:\n",
      "error: Insufficient distinct clusters formed\n",
      "n_clusters_formed: 1\n",
      "cluster_sizes: {np.int64(2): np.int64(39053)}\n",
      "score_range: {'min': np.float64(-0.013055160215452055), 'max': np.float64(0.588711862057856)}\n",
      "\n",
      "Local Outlier Factor Metrics:\n",
      "silhouette_score: 0.985477222911455\n",
      "calinski_harabasz_score: 639.5620796425093\n",
      "davies_bouldin_score: 2.528040757502514\n",
      "diagnostics: {'n_clusters': 2, 'cluster_sizes': {np.int64(0): np.int64(170), np.int64(2): np.int64(38883)}, 'cluster_proportions': {np.int64(0): np.float64(0.004353058663867052), np.int64(2): np.float64(0.995646941336133)}, 'score_distribution': {'min': np.float64(-0.5719205514496825), 'max': np.float64(90639999999.46509), 'mean': np.float64(6835838.003138249), 'std': np.float64(779972270.9915607)}}\n",
      "\n",
      "Autoencoder Metrics:\n",
      "error: Insufficient distinct clusters formed\n",
      "n_clusters_formed: 1\n",
      "cluster_sizes: {np.int64(2): np.int64(39053)}\n",
      "score_range: {'min': np.float64(0.06985746443447098), 'max': np.float64(1329052098.859882)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Isolation Forest\n",
    "iso_metrics = evaluate_model_unsupervised(X_scaled, iso_scores)\n",
    "print(\"Isolation Forest Metrics:\")\n",
    "for k, v in iso_metrics.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Evaluate LOF\n",
    "lof_metrics = evaluate_model_unsupervised(X_scaled, lof_scores)\n",
    "print(\"\\nLocal Outlier Factor Metrics:\")\n",
    "for k, v in lof_metrics.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Evaluate Autoencoder\n",
    "ae_metrics = evaluate_model_unsupervised(X_scaled, ae_scores)\n",
    "print(\"\\nAutoencoder Metrics:\")\n",
    "for k, v in ae_metrics.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all three models\n",
    "joblib.dump(iso_forest_model, f\"{output_directory}/iso_forest_model.pkl\")\n",
    "joblib.dump(lof_model, f\"{output_directory}/lof_model.pkl\")\n",
    "autoencoder_model.save(f\"{output_directory}/autoencoder_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
